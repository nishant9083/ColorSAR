{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeL4YiHy-HFx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io, color\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctdQ4-JK-tYD"
      },
      "source": [
        "## Step 1: Load and Slice Images into Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbOpcgmC-rkT"
      },
      "outputs": [],
      "source": [
        "def load_and_slice_image(image_path, patch_size=224):\n",
        "    img = load_img(image_path)\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    patches = []\n",
        "    img_height, img_width, _ = img_array.shape\n",
        "\n",
        "    for i in range(0, img_height, patch_size):\n",
        "        for j in range(0, img_width, patch_size):\n",
        "            patch = img_array[i:i+patch_size, j:j+patch_size]\n",
        "            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n",
        "                patches.append(patch)\n",
        "\n",
        "    return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTwt_UTJ-zKm"
      },
      "source": [
        "## Step 2: Convert Patches to Lab Color Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMw6bAnH-2Dl"
      },
      "outputs": [],
      "source": [
        "def convert_to_lab(patches):\n",
        "    lab_patches = []\n",
        "    for patch in patches:\n",
        "        lab_patch = color.rgb2lab(patch / 255.0)  # Convert RGB to Lab\n",
        "        lab_patches.append(lab_patch)\n",
        "\n",
        "    return lab_patches\n",
        "\n",
        "def prepare_data_for_training(lab_patches):\n",
        "    L = []\n",
        "    ab = []\n",
        "\n",
        "    for lab_patch in lab_patches:\n",
        "        L.append(lab_patch[:,:,0])  # L channel\n",
        "        ab.append(lab_patch[:,:,1:])  # ab channels\n",
        "\n",
        "    L = np.array(L)\n",
        "    ab = np.array(ab)\n",
        "\n",
        "    # Normalize the data to [-1, 1]\n",
        "    L = (L - 50) / 50.0  # L channel normalization\n",
        "    ab = ab / 128.0  # ab channels normalization\n",
        "\n",
        "    return L[..., np.newaxis], ab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights, DenseNet121_Weights\n",
        "\n",
        "class EnsembleEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnsembleEncoder, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet50 and DenseNet121\n",
        "        self.resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.densenet121 = models.densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
        "\n",
        "        # Remove the fully connected layers\n",
        "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
        "        # self.densenet121 = nn.Sequential(*list(self.densenet121.children())[:-1])\n",
        "        self.densenet121.classifier = nn.Identity()  # Remove the fully connected layer\n",
        "\n",
        "\n",
        "        # Custom layers for fusion\n",
        "        self.conv1x1_resnet50 = nn.ModuleList([\n",
        "            nn.Conv2d(256, 128, kernel_size=1),\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.Conv2d(2048, 1024, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        self.conv1x1_densenet121 = nn.ModuleList([\n",
        "            nn.Conv2d(256, 128, kernel_size=1),\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        # Fusion blocks\n",
        "        self.fusion_blocks = nn.ModuleList([\n",
        "            self.fusion_block(128, 128),\n",
        "            self.fusion_block(256, 256),\n",
        "            self.fusion_block(512, 512),\n",
        "            self.fusion_block(1024, 1024)\n",
        "        ])\n",
        "\n",
        "    # Fusion block\n",
        "    def fusion_block(self, in_channels_resnet, in_channels_densenet):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels_resnet + in_channels_densenet, in_channels_resnet, kernel_size=1),\n",
        "            nn.BatchNorm2d(in_channels_resnet),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through ResNet50\n",
        "        resnet_features = []\n",
        "        resnet_input = x  # The input grayscale image, repeated 3 times\n",
        "        for i, layer in enumerate(self.resnet50.children()):\n",
        "            resnet_input = layer(resnet_input)\n",
        "            # print('resnet input', resnet_input.shape)\n",
        "            if i in [4, 5, 6, 7]:  # Extract features after specific layers\n",
        "                resnet_features.append(self.conv1x1_resnet50[i-4](resnet_input))\n",
        "\n",
        "        # Forward pass through DenseNet121\n",
        "        densenet_features = []\n",
        "        idx = 0\n",
        "        densenet_input = x  # The same input grayscale image\n",
        "        for i, layer in enumerate(self.densenet121.features):\n",
        "            # print(layer)\n",
        "            densenet_input = layer(densenet_input)\n",
        "            # print('densenet input', densenet_input.shape)\n",
        "            if i in [ 4, 6, 8, 11]:  # After each dense block\n",
        "                densenet_features.append(self.conv1x1_densenet121[idx](densenet_input))\n",
        "                idx += 1\n",
        "\n",
        "        # Fusion of features from both networks\n",
        "        # print(f\"ResNet features: {[f.shape for f in resnet_features]}\")\n",
        "        # print(f\"DenseNet features: {[f.shape for f in densenet_features]}\")\n",
        "        fused_features = []\n",
        "        for i in range(4):\n",
        "            # fused = (resnet_features[i] + densenet_features[i]) / 2 # average fusion\n",
        "            # fused, _ = torch.max(torch.stack([resnet_features[i], densenet_features[i]]), dim=0)  # Max Fusion\n",
        "            fused = torch.cat((resnet_features[i], densenet_features[i]), dim=1)\n",
        "            fused = self.fusion_blocks[i](fused)\n",
        "            fused_features.append(fused)\n",
        "\n",
        "        return fused_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Decoder block 1: Takes input from Fusion Block 4\n",
        "        self.decode1 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 7x7 -> 14x14\n",
        "        )\n",
        "\n",
        "        # Decoder block 2: Takes input from Decoder Block 1 + Fusion Block 3 (512 + 512 channels)\n",
        "        self.decode2 = nn.Sequential(\n",
        "            nn.Conv2d(512 + 512, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 14x14 -> 28x28\n",
        "        )\n",
        "\n",
        "        # Decoder block 3: Takes input from Decoder Block 2 + Fusion Block 2 (256 + 256 channels)\n",
        "        self.decode3 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 28x28 -> 56x56\n",
        "        )\n",
        "\n",
        "        # Decoder block 4: Takes input from Decoder Block 3 + Fusion Block 1 (128 + 128 channels)\n",
        "        self.decode4 = nn.Sequential(\n",
        "            nn.Conv2d(128 + 128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 56x56 -> 112x112\n",
        "        )\n",
        "\n",
        "        # Final decoder block: Reduce to 2 channels (ab channels)\n",
        "        self.decode5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),  # Output in the range [-1, 1]\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 112x112 -> 224x224\n",
        "        )\n",
        "\n",
        "    def forward(self, features_7x7, features_14x14, features_28x28, features_56x56):\n",
        "        x = self.decode1(features_7x7)  # Output of Fusion Block 4\n",
        "        x = torch.cat([x, features_14x14], dim=1)  # Skip connection with Fusion Block 3\n",
        "        x = self.decode2(x)  # Output of Decoder Block 1\n",
        "\n",
        "        x = torch.cat([x, features_28x28], dim=1)  # Skip connection with Fusion Block 2\n",
        "        x = self.decode3(x)  # Output of Decoder Block 2\n",
        "\n",
        "        x = torch.cat([x, features_56x56], dim=1)  # Skip connection with Fusion Block 1\n",
        "        x = self.decode4(x)  # Output of Decoder Block 3\n",
        "\n",
        "        output = self.decode5(x)  # Final output\n",
        "\n",
        "        return output\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
