{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeL4YiHy-HFx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import io, color\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctdQ4-JK-tYD"
      },
      "source": [
        "## Step 1: Load and Slice Images into Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbOpcgmC-rkT"
      },
      "outputs": [],
      "source": [
        "def load_and_slice_image(image_path, patch_size=224):\n",
        "    img = load_img(image_path)\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    patches = []\n",
        "    img_height, img_width, _ = img_array.shape\n",
        "\n",
        "    for i in range(0, img_height, patch_size):\n",
        "        for j in range(0, img_width, patch_size):\n",
        "            patch = img_array[i:i+patch_size, j:j+patch_size]\n",
        "            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:\n",
        "                patches.append(patch)\n",
        "\n",
        "    return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTwt_UTJ-zKm"
      },
      "source": [
        "## Step 2: Convert Patches to Lab Color Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMw6bAnH-2Dl"
      },
      "outputs": [],
      "source": [
        "def convert_to_lab(patches):\n",
        "    lab_patches = []\n",
        "    for patch in patches:\n",
        "        lab_patch = color.rgb2lab(patch / 255.0)  # Convert RGB to Lab\n",
        "        lab_patches.append(lab_patch)\n",
        "\n",
        "    return lab_patches\n",
        "\n",
        "def prepare_data_for_training(lab_patches):\n",
        "    L = []\n",
        "    ab = []\n",
        "\n",
        "    for lab_patch in lab_patches:\n",
        "        L.append(lab_patch[:,:,0])  # L channel\n",
        "        ab.append(lab_patch[:,:,1:])  # ab channels\n",
        "\n",
        "    L = np.array(L)\n",
        "    ab = np.array(ab)\n",
        "\n",
        "    # Normalize the data to [-1, 1]\n",
        "    L = (L - 50) / 50.0  # L channel normalization\n",
        "    ab = ab / 128.0  # ab channels normalization\n",
        "\n",
        "    return L[..., np.newaxis], ab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Combine and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path):\n",
        "    patches = load_and_slice_image(image_path)\n",
        "    lab_patches = convert_to_lab(patches)\n",
        "    L, ab = prepare_data_for_training(lab_patches)\n",
        "    return L, ab\n",
        "\n",
        "# Example usage\n",
        "image_path = 'path_to_your_image.jpg'\n",
        "L, ab = load_and_preprocess_image(image_path)\n",
        "\n",
        "print(\"L shape:\", L.shape)\n",
        "print(\"ab shape:\", ab.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading Data in Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class ImageDataGenerator(Sequence):\n",
        "    def __init__(self, image_paths, batch_size=32, patch_size=224):\n",
        "        self.image_paths = image_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_paths) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_paths = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        L_batch = []\n",
        "        ab_batch = []\n",
        "\n",
        "        for image_path in batch_paths:\n",
        "            patches = load_and_slice_image(image_path, patch_size=self.patch_size)\n",
        "            lab_patches = convert_to_lab(patches)\n",
        "            L, ab = prepare_data_for_training(lab_patches)\n",
        "            L_batch.append(L)\n",
        "            ab_batch.append(ab)\n",
        "\n",
        "        return np.concatenate(L_batch, axis=0), np.concatenate(ab_batch, axis=0)\n",
        "\n",
        "# Example usage\n",
        "image_directory = 'path_to_your_image_directory'\n",
        "image_paths = [os.path.join(image_directory, fname) for fname in os.listdir(image_directory)]\n",
        "\n",
        "data_gen = ImageDataGenerator(image_paths, batch_size=32)\n",
        "\n",
        "# Fetch a batch\n",
        "L_batch, ab_batch = data_gen[0]\n",
        "print(\"L_batch shape:\", L_batch.shape)\n",
        "print(\"ab_batch shape:\", ab_batch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights, DenseNet121_Weights\n",
        "\n",
        "class EnsembleEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EnsembleEncoder, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet50 and DenseNet121\n",
        "        self.resnet50 = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "        self.densenet121 = models.densenet121(weights=DenseNet121_Weights.DEFAULT)\n",
        "\n",
        "        # Remove the fully connected layers\n",
        "        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
        "        # self.densenet121 = nn.Sequential(*list(self.densenet121.children())[:-1])\n",
        "        self.densenet121.classifier = nn.Identity()  # Remove the fully connected layer\n",
        "\n",
        "\n",
        "        # Custom layers for fusion\n",
        "        self.conv1x1_resnet50 = nn.ModuleList([\n",
        "            nn.Conv2d(256, 128, kernel_size=1),\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.Conv2d(2048, 1024, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        self.conv1x1_densenet121 = nn.ModuleList([\n",
        "            nn.Conv2d(256, 128, kernel_size=1),\n",
        "            nn.Conv2d(512, 256, kernel_size=1),\n",
        "            nn.Conv2d(1024, 512, kernel_size=1),\n",
        "            nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "        ])\n",
        "\n",
        "        # Fusion blocks\n",
        "        self.fusion_blocks = nn.ModuleList([\n",
        "            self.fusion_block(128, 128),\n",
        "            self.fusion_block(256, 256),\n",
        "            self.fusion_block(512, 512),\n",
        "            self.fusion_block(1024, 1024)\n",
        "        ])\n",
        "\n",
        "    # Fusion block\n",
        "    def fusion_block(self, in_channels_resnet, in_channels_densenet):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels_resnet + in_channels_densenet, in_channels_resnet, kernel_size=1),\n",
        "            nn.BatchNorm2d(in_channels_resnet),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through ResNet50\n",
        "        resnet_features = []\n",
        "        resnet_input = x  # The input grayscale image, repeated 3 times\n",
        "        for i, layer in enumerate(self.resnet50.children()):\n",
        "            resnet_input = layer(resnet_input)\n",
        "            # print('resnet input', resnet_input.shape)\n",
        "            if i in [4, 5, 6, 7]:  # Extract features after specific layers\n",
        "                resnet_features.append(self.conv1x1_resnet50[i-4](resnet_input))\n",
        "\n",
        "        # Forward pass through DenseNet121\n",
        "        densenet_features = []\n",
        "        idx = 0\n",
        "        densenet_input = x  # The same input grayscale image\n",
        "        for i, layer in enumerate(self.densenet121.features):\n",
        "            # print(layer)\n",
        "            densenet_input = layer(densenet_input)\n",
        "            # print('densenet input', densenet_input.shape)\n",
        "            if i in [ 4, 6, 8, 11]:  # After each dense block\n",
        "                densenet_features.append(self.conv1x1_densenet121[idx](densenet_input))\n",
        "                idx += 1\n",
        "\n",
        "        # Fusion of features from both networks\n",
        "        # print(f\"ResNet features: {[f.shape for f in resnet_features]}\")\n",
        "        # print(f\"DenseNet features: {[f.shape for f in densenet_features]}\")\n",
        "        fused_features = []\n",
        "        for i in range(4):\n",
        "            # fused = (resnet_features[i] + densenet_features[i]) / 2 # average fusion\n",
        "            # fused, _ = torch.max(torch.stack([resnet_features[i], densenet_features[i]]), dim=0)  # Max Fusion\n",
        "            fused = torch.cat((resnet_features[i], densenet_features[i]), dim=1)\n",
        "            fused = self.fusion_blocks[i](fused)\n",
        "            fused_features.append(fused)\n",
        "\n",
        "        return fused_features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Decoder block 1: Takes input from Fusion Block 4\n",
        "        self.decode1 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 7x7 -> 14x14\n",
        "        )\n",
        "\n",
        "        # Decoder block 2: Takes input from Decoder Block 1 + Fusion Block 3 (512 + 512 channels)\n",
        "        self.decode2 = nn.Sequential(\n",
        "            nn.Conv2d(512 + 512, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 14x14 -> 28x28\n",
        "        )\n",
        "\n",
        "        # Decoder block 3: Takes input from Decoder Block 2 + Fusion Block 2 (256 + 256 channels)\n",
        "        self.decode3 = nn.Sequential(\n",
        "            nn.Conv2d(256 + 256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 28x28 -> 56x56\n",
        "        )\n",
        "\n",
        "        # Decoder block 4: Takes input from Decoder Block 3 + Fusion Block 1 (128 + 128 channels)\n",
        "        self.decode4 = nn.Sequential(\n",
        "            nn.Conv2d(128 + 128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 56x56 -> 112x112\n",
        "        )\n",
        "\n",
        "        # Final decoder block: Reduce to 2 channels (ab channels)\n",
        "        self.decode5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 2, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),  # Output in the range [-1, 1]\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)  # 112x112 -> 224x224\n",
        "        )\n",
        "\n",
        "    def forward(self, features_7x7, features_14x14, features_28x28, features_56x56):\n",
        "        x = self.decode1(features_7x7)  # Output of Fusion Block 4\n",
        "        x = torch.cat([x, features_14x14], dim=1)  # Skip connection with Fusion Block 3\n",
        "        x = self.decode2(x)  # Output of Decoder Block 1\n",
        "\n",
        "        x = torch.cat([x, features_28x28], dim=1)  # Skip connection with Fusion Block 2\n",
        "        x = self.decode3(x)  # Output of Decoder Block 2\n",
        "\n",
        "        x = torch.cat([x, features_56x56], dim=1)  # Skip connection with Fusion Block 1\n",
        "        x = self.decode4(x)  # Output of Decoder Block 3\n",
        "\n",
        "        output = self.decode5(x)  # Final output\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Freeze the encoder parameters\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "model = ColorizationModel(encoder, decoder).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Progress bar for training\n",
        "    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\")\n",
        "\n",
        "    for i, (L_batch, ab_batch) in enumerate(train_bar):\n",
        "        L, ab = L_batch.to(device), ab_batch.to(device)\n",
        "        L = L.repeat(1, 3, 1, 1)  # Repeat grayscale image to 3 channels\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(L)\n",
        "        loss = criterion(output, ab)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        train_bar.set_postfix(loss=f\"{running_loss/(i+1):.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # Progress bar for validation\n",
        "    val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (L_batch, ab_batch) in enumerate(val_bar):\n",
        "            L, ab = L_batch.to(device), ab_batch.to(device)\n",
        "            L = L.repeat(1, 3, 1, 1)  # Repeat grayscale image to 3 channels\n",
        "            output = model(L)\n",
        "            loss = criterion(output, ab)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            val_bar.set_postfix(loss=f\"{val_loss/(i+1):.4f}\")\n",
        "\n",
        "    # Print statistics and save the best model\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_colorization_model.pth')\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model, 'color_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "# Create indices for the test subset\n",
        "test_indices = list(range(17000, 18000))  # Select 1000 images for testing\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!kaggle datasets download -d requiemonk/sentinel12-image-pairs-segregated-by-terrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip sentinel12-image-pairs-segregated-by-terrain.zip\n",
        "!rm sentinel12-image-pairs-segregated-by-terrain.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "opt = []\n",
        "sar = []\n",
        "root_dir = './v_2'\n",
        "for dir in os.listdir(root_dir):\n",
        "  path = os.path.join(root_dir, dir)\n",
        "  s1, s2  = os.listdir(path)\n",
        "  for file in os.listdir(os.path.join(path, s1)):\n",
        "    if file.endswith('.png'):\n",
        "      sar.append(os.path.join(path, s1, file))\n",
        "  for file in os.listdir(os.path.join(path, s2)):\n",
        "    if file.endswith('.png'):\n",
        "      opt.append(os.path.join(path, s2, file))\n",
        "opt = sorted(opt)\n",
        "sar = sorted(sar)\n",
        "print(len(opt), len(sar))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "class ColorizationDatasetNew(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        color_img = cv2.imread(image_path)\n",
        "        gray_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE).astype(\"float32\")  # Convert to grayscale\n",
        "        gray_img = cv2.resize(gray_img, (224, 224))\n",
        "        gray_img = gray_img / 255. - 1\n",
        "        # change shape to channel shape\n",
        "        gray_img = gray_img.reshape(224, 224, 1)\n",
        "        gray_img = transforms.ToTensor()(gray_img)\n",
        "        # print(gray_img.shape)\n",
        "\n",
        "        # Apply desired transformations\n",
        "        color_img = cv2.resize(color_img, (224, 224))\n",
        "        # gray_img_3channel = cv2.merge([gray_img, gray_img, gray_img])\n",
        "\n",
        "        # Convert to Lab color space\n",
        "        L_channel, ab_channels = rgb_to_lab(color_img)  # Assuming rgb_to_lab is defined\n",
        "        # print(L_channel.shape)\n",
        "\n",
        "        # if self.transform:\n",
        "        #     L_channel = self.transform(L_channel)\n",
        "        #     ab_channels = self.transform(ab_channels)\n",
        "\n",
        "        return gray_img, ab_channels\n",
        "\n",
        "\n",
        "# Define transformations (if needed)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add other transformations as required\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "dataset_new = ColorizationDatasetNew(opt[:4000], transform=transform)\n",
        "\n",
        "# Create the DataLoader\n",
        "test_set = DataLoader(dataset_new, batch_size=32, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gray = 'despeckled_image.png'\n",
        "sar = 'img.png'\n",
        "opt = \"img_opt.png\"\n",
        "\n",
        "gray_img = cv2.imread(gray)\n",
        "sar_img = cv2.imread(sar)\n",
        "opt_img = cv2.imread(opt)\n",
        "# resize to 224\n",
        "gray_img = cv2.resize(gray_img, (224, 224))\n",
        "sar_img = cv2.resize(sar_img, (224, 224))\n",
        "opt_img = cv2.resize(opt_img, (224, 224))\n",
        "# convert to lab\n",
        "L_channel, ab_channels = rgb_to_lab(gray_img)\n",
        "L_channel_sar, ab_channels_sar = rgb_to_lab(sar_img)\n",
        "L_channel_opt, ab_channels_opt = rgb_to_lab(opt_img)\n",
        "# add batch dim\n",
        "L_channel = L_channel.unsqueeze(0)\n",
        "L_channel_sar = L_channel_sar.unsqueeze(0)\n",
        "L_channel_opt = L_channel_opt.unsqueeze(0)\n",
        "ab_channels_opt = ab_channels_opt.unsqueeze(0)\n",
        "ab_channels_sar = ab_channels_sar.unsqueeze(0)\n",
        "# repeat L channel to 3 channel\n",
        "L_channel = L_channel.repeat(1, 3, 1, 1)\n",
        "L_channel_sar = L_channel_sar.repeat(1, 3, 1, 1)\n",
        "L_channel_opt = L_channel_opt.repeat(1, 3, 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "# Load your trained model\n",
        "model = ColorizationModel(encoder, decoder)\n",
        "model.load_state_dict(torch.load('best_colorization_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    predicted_ab = model(L_channel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install PyWavelets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from skimage import img_as_ubyte, io, img_as_float\n",
        "from skimage.restoration import (\n",
        "    denoise_nl_means,\n",
        "    denoise_tv_chambolle,\n",
        "    denoise_wavelet,\n",
        "    denoise_bilateral\n",
        ")\n",
        "from skimage.restoration import estimate_sigma\n",
        "import numpy as np\n",
        "from skimage.util import random_noise\n",
        "import cv2\n",
        "\n",
        "def denoise_image(image):\n",
        "    # Convert image to float\n",
        "    image = img_as_float(image)\n",
        "\n",
        "    # Estimate sigma for NLM\n",
        "    sigma_est = np.mean(estimate_sigma(image))\n",
        "\n",
        "    # Apply NLM and its combinations\n",
        "    denoised_image_nlm = denoise_nl_means(image, h=1.0 * sigma_est, fast_mode=True, patch_size=7, patch_distance=11, channel_axis=None)\n",
        "\n",
        "    # Combinations with NLM\n",
        "    denoised_image_nlm_tv = denoise_tv_chambolle(denoise_nl_means(image, h=1.0 * sigma_est, fast_mode=True, patch_size=7, patch_distance=11, channel_axis=None), weight=0.1)\n",
        "\n",
        "    denoised_image_nlm_wavelet = denoise_wavelet(denoise_nl_means(image, h=1.0 * sigma_est, fast_mode=True, patch_size=7, patch_distance=11, channel_axis=None), method='BayesShrink', mode='soft')\n",
        "\n",
        "    denoised_image_nlm_bilateral = denoise_bilateral(denoise_nl_means(image, h=1.0 * sigma_est, fast_mode=True, patch_size=7, patch_distance=11, channel_axis=None), sigma_color=0.05, sigma_spatial=15)\n",
        "\n",
        "    # Combinations of the above\n",
        "    denoised_image_nlm_tv_wavelet = denoise_wavelet(denoise_tv_chambolle(denoise_nl_means(image, h=1.0 * sigma_est, fast_mode=True, patch_size=7, patch_distance=11, channel_axis=None), weight=0.1), method='BayesShrink', mode='soft')\n",
        "\n",
        "    denoised_image_nlm_tv_bilateral = denoise_bilateral(denoise_tv_chambolle(denoise_nl_means(image, h=1.0 * sigma_est, fast_mode=True, patch_size=7, patch_distance=11, channel_axis=None), weight=0.1), sigma_color=0.05, sigma_spatial=15)\n",
        "\n",
        "    # Return one of the denoised images as required\n",
        "    return denoised_image_nlm_tv_bilateral\n",
        "\n",
        "# Example usage:\n",
        "image = io.imread('img.png', as_gray=True)\n",
        "denoised_image = denoise_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "denoised_image = denoised_image * 255\n",
        "denoised_image = denoised_image.astype(np.uint8)\n",
        "# save the image\n",
        "cv2.imwrite('despeckled_image.png', denoised_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a batch from the test loader\n",
        "dataiter = iter(test_loader)\n",
        "L_batch, ab_batch = next(dataiter)\n",
        "L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
        "L_batch = L_batch.repeat(1, 3, 1, 1)\n",
        "\n",
        "# for idx, (L_batch, ab_batch) in enumerate(test_set):\n",
        "#     L_batch, ab_batch = L_batch.to(device), ab_batch.to(device)\n",
        "    # L_batch = L_batch.repeat(1, 3, 1, 1)\n",
        "# Get predictions\n",
        "# Load your trained model\n",
        "# model1 = ColorizationModel(encoder, decoder)\n",
        "# model1.load_state_dict(torch.load('best_colorization_model.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "# model1.eval()\n",
        "with torch.no_grad():\n",
        "   predicted_ab = model(L_batch)\n",
        "    # if idx == 0:\n",
        "    #   break\n",
        "    # break\n",
        "# print(predicted_ab[6])\n",
        "# print(ab_batch[6])\n",
        "# De-normalize ab channels\n",
        "# predicted_ab = predicted_ab * 128.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L_batch = L_channel[:, 0, :, :]\n",
        "L_batch = L_batch.unsqueeze(1)  # Add channel dimension\n",
        "\n",
        "# Assuming L_batch is in the range [-1, 1] and ab_batch is in the range [-1, 1]\n",
        "L_batch = (L_batch + 1) * 50  # Add 1 and multiply by 50\n",
        "predicted_ab = predicted_ab * 110  # Multiply by 110\n",
        "ab_batch = ab_channels_opt * 110\n",
        "\n",
        "# Combine L and ab channels\n",
        "predicted_lab = torch.cat([L_batch, predicted_ab], dim=1)\n",
        "real_lab = torch.cat([L_batch, ab_batch], dim=1)\n",
        "\n",
        "# Assuming predicted_lab is on the CPU and has shape (batch_size, 3, height, width)\n",
        "predicted_lab = predicted_lab.cpu().numpy()\n",
        "real_lab = real_lab.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predicted_ab[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from skimage.color import lab2rgb\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "# Iterate over the batch\n",
        "for i in range(predicted_lab.shape[0]):\n",
        "    # Extract the Lab image for the current batch element\n",
        "    lab_image = predicted_lab[i]\n",
        "    real_img = real_lab[i]\n",
        "\n",
        "    # Transpose to (height, width, 3) for skimage\n",
        "    lab_image = lab_image.transpose(1, 2, 0)\n",
        "    real_img = real_img.transpose(1, 2, 0)\n",
        "\n",
        "    # Convert Lab to RGB using skimage.color.lab2rgb\n",
        "    rgb_image = lab2rgb(lab_image)\n",
        "    real_rgb = lab2rgb(real_img)\n",
        "\n",
        "    # Do something with the rgb_image (e.g., display it, save it, etc.)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(real_rgb)\n",
        "    plt.axis('off')\n",
        "    plt.title('Real Image')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(rgb_image)\n",
        "    plt.title('Predicted Image')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert Lab to RGB\n",
        "predicted_rgb_images = []\n",
        "for lab_image in predicted_lab:\n",
        "    lab_image_np = lab_image.transpose(1, 2, 0).astype('uint8')\n",
        "    rgb_image_np = cv2.cvtColor(lab_image_np, cv2.COLOR_Lab2RGB)\n",
        "    predicted_rgb_images.append(rgb_image_np)\n",
        "\n",
        "# Display images\n",
        "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
        "for i, img in enumerate(predicted_rgb_images):\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].axis('off')\n",
        "    if i==3:\n",
        "        break\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics:\n",
        "\n",
        "Importing Libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the necessary libraries if not already installed\n",
        "pip install scikit-image lpips opencv-python\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2  # for reading images\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import lpips\n",
        "import torch\n",
        "\n",
        "# Initialize LPIPS model\n",
        "lpips_fn = lpips.LPIPS(net='alex')  # Can also use 'vgg' or 'squeeze'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading DIV2K images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths to the dataset (update these according to your dataset location)\n",
        "HR_PATH = 'path_to_DIV2K_HR'  # Path to high-resolution images\n",
        "SR_PATH = 'path_to_SR_images'  # Path to your super-resolved or reconstructed images\n",
        "\n",
        "# List of image filenames (assuming .png or .jpg)\n",
        "hr_images = sorted([f for f in os.listdir(HR_PATH) if f.endswith('.png') or f.endswith('.jpg')])\n",
        "sr_images = sorted([f for f in os.listdir(SR_PATH) if f.endswith('.png') or f.endswith('.jpg')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate PSNR and SSIM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_psnr_ssim(hr_img, sr_img):\n",
        "    # PSNR\n",
        "    psnr_value = psnr(hr_img, sr_img, data_range=hr_img.max() - hr_img.min())\n",
        "\n",
        "    # SSIM\n",
        "    ssim_value = ssim(hr_img, sr_img, multichannel=True)\n",
        "\n",
        "    return psnr_value, ssim_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculate LPIPS:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_lpips(hr_img, sr_img):\n",
        "    # Convert to PyTorch tensors (assuming images are in range [0, 255])\n",
        "    hr_tensor = torch.tensor(hr_img).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "    sr_tensor = torch.tensor(sr_img).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "\n",
        "    # LPIPS score\n",
        "    lpips_value = lpips_fn(hr_tensor, sr_tensor)\n",
        "\n",
        "    return lpips_value.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full Process to compute metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop through each pair of images\n",
        "psnr_scores = []\n",
        "ssim_scores = []\n",
        "lpips_scores = []\n",
        "\n",
        "for hr_img_file, sr_img_file in zip(hr_images, sr_images):\n",
        "    # Load images using OpenCV (BGR -> RGB conversion)\n",
        "    hr_img = cv2.imread(os.path.join(HR_PATH, hr_img_file))\n",
        "    sr_img = cv2.imread(os.path.join(SR_PATH, sr_img_file))\n",
        "\n",
        "    hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
        "    sr_img = cv2.cvtColor(sr_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize if necessary to ensure both images have the same dimensions\n",
        "    if hr_img.shape != sr_img.shape:\n",
        "        sr_img = cv2.resize(sr_img, (hr_img.shape[1], hr_img.shape[0]))\n",
        "\n",
        "    # Calculate PSNR and SSIM\n",
        "    psnr_value, ssim_value = calculate_psnr_ssim(hr_img, sr_img)\n",
        "\n",
        "    # Calculate LPIPS\n",
        "    lpips_value = calculate_lpips(hr_img, sr_img)\n",
        "\n",
        "    # Store results\n",
        "    psnr_scores.append(psnr_value)\n",
        "    ssim_scores.append(ssim_value)\n",
        "    lpips_scores.append(lpips_value)\n",
        "\n",
        "# Calculate average scores\n",
        "avg_psnr = np.mean(psnr_scores)\n",
        "avg_ssim = np.mean(ssim_scores)\n",
        "avg_lpips = np.mean(lpips_scores)\n",
        "\n",
        "print(f\"Average PSNR: {avg_psnr:.4f}\")\n",
        "print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "print(f\"Average LPIPS: {avg_lpips:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load CIFAR-10 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries if not already installed\n",
        "pip install torch torchvision\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "# Define transforms (normalization, convert to tensor)\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# Load the CIFAR-10 test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader for testset\n",
        "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR-10\n",
        "classes = testset.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute MAE for Images (Pixel-wise MAE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mae_images(gt_img, pred_img):\n",
        "    \"\"\"\n",
        "    Compute MAE between ground truth (gt_img) and predicted image (pred_img).\n",
        "    Both inputs should be PyTorch tensors.\n",
        "    \"\"\"\n",
        "    # Ensure they are on the same device (e.g., CPU) and convert to float\n",
        "    gt_img = gt_img.float()\n",
        "    pred_img = pred_img.float()\n",
        "    \n",
        "    # Compute the absolute difference and mean it\n",
        "    mae_value = torch.mean(torch.abs(gt_img - pred_img))\n",
        "    \n",
        "    return mae_value.item()  # Return as a scalar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example Usage for single batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage (assuming ground truth and predicted images are in testloader)\n",
        "for i, data in enumerate(testloader):\n",
        "    images, labels = data  # Ground truth images and labels\n",
        "    pred_images = model(images)  # Assuming a model generates reconstructed images\n",
        "\n",
        "    # Compute MAE for the batch\n",
        "    batch_mae = compute_mae_images(images, pred_images)\n",
        "    \n",
        "    print(f\"Batch {i+1} MAE: {batch_mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute MAE for Labels (for classifications):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_mae_labels(gt_labels, pred_labels):\n",
        "    \"\"\"\n",
        "    Compute MAE between ground truth (gt_labels) and predicted labels (pred_labels).\n",
        "    Both inputs should be PyTorch tensors.\n",
        "    \"\"\"\n",
        "    # Ensure the predicted labels are of the same shape and type\n",
        "    mae_value = torch.mean(torch.abs(gt_labels - pred_labels.float()))\n",
        "    return mae_value.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example Usage for Label Predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming you have a trained classification model\n",
        "correct_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Loop through the test dataset\n",
        "for images, labels in testloader:\n",
        "    # Predict using the model (assuming output is logits)\n",
        "    outputs = model(images)\n",
        "    \n",
        "    # Get the predicted class (highest probability)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    \n",
        "    # Store labels and predictions\n",
        "    correct_labels.append(labels)\n",
        "    predicted_labels.append(predicted)\n",
        "\n",
        "# Convert lists to tensors\n",
        "correct_labels = torch.cat(correct_labels)\n",
        "predicted_labels = torch.cat(predicted_labels)\n",
        "\n",
        "# Compute MAE for labels\n",
        "mae_labels = compute_mae_labels(correct_labels, predicted_labels)\n",
        "print(f\"MAE for labels: {mae_labels}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
